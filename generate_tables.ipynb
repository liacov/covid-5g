{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set language (it or en)\n",
    "LANG = 'it' \n",
    "\n",
    "# set paths\n",
    "NET_5G_IT_PATH = 'data/network/net_5g_{}'.format(LANG)\n",
    "NET_COVID_IT_PATH = 'data/network/net_5gANDcovid_{}'.format(LANG)\n",
    "NET_5GANDCOVID_IT_PATH = 'data/network/net_covid_{}'.format(LANG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the seed hashtags, the 5g network has a largest component of size 1463, 89% of the total.\n",
      "\n",
      "Removing the seed hashtags, the covid network has a largest component of size 295, 100% of the total.\n",
      "\n",
      "Removing the seed hashtags, the 5gANDcovid network has a largest component of size 197, 71% of the total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define networks container, indexed by periods\n",
    "networks = {\n",
    "    '5g': nx.read_gexf(NET_5G_IT_PATH),\n",
    "    'covid': nx.read_gexf(NET_COVID_IT_PATH),\n",
    "    '5gANDcovid': nx.read_gexf(NET_5GANDCOVID_IT_PATH)\n",
    "}\n",
    "\n",
    "seed_list = {\n",
    "    '5g': ['5g'],\n",
    "    'covid': ['covid','covid19','coronavirus'],\n",
    "    '5gANDcovid': ['5g','covid','covid19','coronavirus']\n",
    "}\n",
    "\n",
    "\n",
    "cc = {}\n",
    "for net in networks.keys():\n",
    "    # Remove seed hashtags and save the giant component\n",
    "    networks[net].remove_nodes_from(seed_list[net])\n",
    "    largest_cc = max(nx.connected_components(networks[net]), key=len)\n",
    "    cc[net] = networks[net].subgraph(largest_cc).copy()\n",
    "    ratio = int(len(largest_cc)/len(networks[net].nodes) * 100)\n",
    "    print(f'Removing the seed hashtags, the {net} network has a largest component of size {len(largest_cc)}, {ratio}% of the total.')\n",
    "    print()\n",
    "    \n",
    "    # save degree centrality\n",
    "    deg = dict(cc[net].degree)\n",
    "    deg = {k: deg[k] for k in sorted(deg, key=deg.get, reverse=True)}\n",
    "    deg = pd.DataFrame.from_dict(deg, orient='index', columns=['degree']).reset_index()\n",
    "    deg = deg.rename(columns={'index':'hashtag'})\n",
    "    deg.to_csv('data/network/degree_{}_{}.csv'.format(net, LANG),index=False)\n",
    "    \n",
    "    # Save PageRank centrality\n",
    "    PR = nx.pagerank_numpy(cc[net])\n",
    "    PR = {k: PR[k] for k in sorted(PR, key=PR.get, reverse=True)}\n",
    "    PR = pd.DataFrame.from_dict(PR, orient='index', columns=['PR']).reset_index()\n",
    "    PR = PR.rename(columns={'index':'hashtag'})\n",
    "    PR.to_csv('data/network/pagerank_{}_{}.csv'.format(net, LANG),index=False)\n",
    "    \n",
    "    # Save edgelist\n",
    "    nx.write_edgelist(cc[net],'data/network/net_{}_edgelist_{}.csv'.format(net, LANG),delimiter=',',data=['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text dataframes - debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov, df_5g = pd.DataFrame(),pd.DataFrame()\n",
    "\n",
    "\n",
    "def parse_tweet(retrieved_tweet, datetime_format='%a %b %d %H:%M:%S %z %Y'):\n",
    "    # Initialize parsed tweet object\n",
    "    parsed_tweet = dict()\n",
    "    # Get tweet id\n",
    "    parsed_tweet['tweet_id'] = str(retrieved_tweet.get('id_str'))\n",
    "    # Get tweet date\n",
    "    parsed_tweet['tweet_date'] = datetime.strptime(\n",
    "        retrieved_tweet.get('created_at'),\n",
    "        datetime_format\n",
    "    )\n",
    "    # Initialize parsed tweet text\n",
    "    tweet_text = ''\n",
    "    # Case tweet is a retweet\n",
    "    if 'retweeted_status' in set(retrieved_tweet.keys()):\n",
    "        # Get inner tweet\n",
    "        retrieved_tweet = retrieved_tweet['retweeted_status']\n",
    "    # Check if current tweet is an extended tweet\n",
    "    if 'extended_tweet' in set(retrieved_tweet.keys()):\n",
    "        tweet_text = retrieved_tweet['extended_tweet']['full_text']\n",
    "    # Case current tweet is not an extended one\n",
    "    else:\n",
    "        tweet_text = retrieved_tweet['text']\n",
    "    # Store tweet text\n",
    "    parsed_tweet['tweet_text'] = tweet_text\n",
    "    # Return tweet\n",
    "    return parsed_tweet\n",
    "\n",
    "tweets = list()\n",
    "# Load input file\n",
    "with open('./data/20200215_only_5g_it.jsonl', 'rb') as in_file:\n",
    "    # Loop through each line in input .jsonl formatted file\n",
    "    for retrieved_tweet in json_lines.reader(in_file, broken=True):\n",
    "        # Format retrieved tweet according to inner DataFrame\n",
    "        parsed_tweet = parse_tweet(retrieved_tweet)\n",
    "        # Append parsed tweet to tweets list\n",
    "        tweets.append(parsed_tweet)\n",
    "# Append list of retrieved tweets to inner Dataframe\n",
    "df_5g = df_5g.append(tweets, ignore_index=True)\n",
    "\n",
    "tweets = list()\n",
    "# Load input file\n",
    "with open('./data/20200215_only_covid_it.jsonl', 'rb') as in_file:\n",
    "    # Loop through each line in input .jsonl formatted file\n",
    "    for retrieved_tweet in json_lines.reader(in_file, broken=True):\n",
    "        # Format retrieved tweet according to inner DataFrame\n",
    "        parsed_tweet = parse_tweet(retrieved_tweet)\n",
    "        # Append parsed tweet to tweets list\n",
    "        tweets.append(parsed_tweet)\n",
    "# Append list of retrieved tweets to inner Dataframe\n",
    "df_cov = df_cov.append(tweets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4349, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = pd.DataFrame()\n",
    "common['hashtag'] = list(set(cc['covid'].nodes) & set(cc['5g'].nodes))\n",
    "\n",
    "deg_cov = dict(cc['covid'].degree)\n",
    "PR_cov = nx.pagerank_numpy(cc['covid'])\n",
    "\n",
    "deg_5g = dict(cc['5g'].degree)\n",
    "PR_5g = nx.pagerank_numpy(cc['5g'])\n",
    "\n",
    "\n",
    "l = [deg_cov, deg_5g, PR_cov, PR_5g]\n",
    "lab = ['deg_cov', 'deg_5g', 'PR_cov', 'PR_5g']\n",
    "for i, d in enumerate(l):\n",
    "    d_new = {k: d[k] for k in d if k in common.hashtag.values}\n",
    "    df = pd.DataFrame.from_dict(d_new, orient='index', columns=[lab[i]])\n",
    "    common = pd.merge(common, df, left_on = 'hashtag', right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = common.sort_values(by = 'PR_cov', ascending = False)\n",
    "common = common.sort_values(by = 'deg_cov', ascending = False)\n",
    "common = common.sort_values(by = 'deg_5g', ascending = False)\n",
    "common = common.sort_values(by = 'PR_5g', ascending = False)\n",
    "common.to_csv('data/network/common_hashtags_it.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
